{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    " \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  \n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  \n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import accuracy_score, classification_report  \n",
    "import xgboost as xgb\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Identify and remove outliers\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#preprocess_traffic_data(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_traffic_data(combined_df):  \n",
    "    \"\"\"  \n",
    "    对交通数据进行预处理。  \n",
    "    \n",
    "    :param combined_df: 包含交通数据的 DataFrame  \n",
    "    :param vehicle_counts: 需要标准化的车辆计数列名列表  \n",
    "    :return: 经过预处理的 DataFrame  \n",
    "    \"\"\"  \n",
    "    vehicle_counts = ['CarCount', 'BikeCount', 'BusCount', 'TruckCount']\n",
    "    # 1. 将 'Traffic Situation' 列转换为分类变量并提取编码  \n",
    "    combined_df['Traffic Situation'] = combined_df['Traffic Situation'].astype('category').cat.codes  \n",
    "    \n",
    "    # 2. 从 'Time' 列提取小时信息  \n",
    "    combined_df['Hour'] = pd.to_datetime(combined_df['Time'], format='%I:%M:%S %p').dt.hour  \n",
    "    \n",
    "    # 3. 创建一个布尔列 'Weekend'，判断是否为周末  \n",
    "    combined_df['Weekend'] = combined_df['Day of the week'].isin(['Saturday', 'Sunday'])  \n",
    "    \n",
    "    # 4. 移除异常值  \n",
    "    combined_df = remove_outliers(combined_df, vehicle_counts)  \n",
    "    \n",
    "    # 5. 使用 QuantileTransformer 进行标准化  \n",
    "    scaler = QuantileTransformer(output_distribution='normal')  \n",
    "    combined_df[vehicle_counts] = scaler.fit_transform(combined_df[vehicle_counts])  \n",
    "    \n",
    "    return combined_df  \n",
    "\n",
    "\n",
    "# model_type = sys.argv[3] # 新增model_type变量\n",
    "# model_save_path = sys.argv[2]\n",
    "# input_train_filepath = sys.argv[1]\n",
    "\n",
    "# combined_df = pd.read_csv(\"..\\Traffic_analysis\\Traffic.csv\")\n",
    "# # 使用示例：  \n",
    "# combined_df = preprocess_traffic_data(combined_df) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = pd.read_csv(\"..\\Traffic_analysis\\TrafficTwoMonth.csv\")\n",
    "# 使用示例：  \n",
    "\n",
    "\n",
    "traffic_two_month_df = pd.read_csv(\"..\\Traffic_analysis\\TrafficTwoMonth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([traffic_df, traffic_two_month_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic Situation   \n",
    "3    7200   -normal   \n",
    "0    1970   -heavy   \n",
    "2    1668   -low    \n",
    "1     726   -high    \n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = preprocess_traffic_data(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "\n",
      "Training Set Size: 9251\n",
      "Test Set Size: 2313\n"
     ]
    }
   ],
   "source": [
    "# 显示数据的前几行（可选）  \n",
    "print(\"Original Data:\")  \n",
    "\n",
    "\n",
    "# 分割数据集为训练集和测试集，80% 作为训练集，20% 作为测试集  \n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)  \n",
    "\n",
    "# 显示分割后的数据集大小（可选）  \n",
    "print(f\"\\nTraining Set Size: {train_df.shape[0]}\")  \n",
    "print(f\"Test Set Size: {test_df.shape[0]}\")  \n",
    "\n",
    "# # 保存数据集为 CSV 文件（可选）  \n",
    "# train_df.to_csv('traffic_data_train.csv', index=False)  \n",
    "# test_df.to_csv('traffic_data_test.csv', index=False)  \n",
    "\n",
    "# print(\"\\nTraining and testing datasets have been saved.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_traffic_data(combined_df):  \n",
    "  \n",
    "    # Prepare the features and target\n",
    "    X = combined_df.drop(columns=['Traffic Situation'])\n",
    "    y = combined_df['Traffic Situation']\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Preprocessing pipeline for numeric and categorical features\n",
    "    numeric_features = ['CarCount', 'BikeCount', 'BusCount', 'TruckCount', 'Total', 'Hour']\n",
    "    categorical_features = ['Time', 'Date', 'Day of the week', 'Weekend']\n",
    "    report = None\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])\n",
    "\n",
    "    # 定义各个模型管道  \n",
    "    rf_model = Pipeline(steps=[  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', RandomForestClassifier(random_state=42))  \n",
    "    ])  \n",
    "\n",
    "    xgb_model = Pipeline(steps=[  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', xgb.XGBClassifier(random_state=42))  \n",
    "    ])  \n",
    "\n",
    "    svm_model = Pipeline(steps=[  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', SVC(random_state=42,probability=True))  \n",
    "    ])  \n",
    "\n",
    "    gb_model = Pipeline(steps=[  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', GradientBoostingClassifier(random_state=42))  \n",
    "    ])  \n",
    "\n",
    "       # 添加决策树模型  \n",
    "    dt_model = Pipeline(steps=[  \n",
    "        ('preprocessor', preprocessor),  \n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))  \n",
    "    ]) \n",
    "        # 训练随机森林模型  \n",
    "    dt_model.fit(X_train, y_train)\n",
    "        # 评估模型表现  \n",
    "    df_y_pred = dt_model.predict(X_test)\n",
    "    print(\"****************************************************************\")\n",
    "    print(\"Random Forest Model Accuracy:\", accuracy_score(y_test, df_y_pred))  \n",
    "    print(\"Random Forest Classification Report:\")  \n",
    "    print(classification_report(y_test, df_y_pred))  \n",
    "    print(\"****************************************************************\")\n",
    "    report = classification_report(y_test, df_y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************\n",
      "Random Forest Model Accuracy: 0.9983792544570502\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       333\n",
      "           1       0.97      1.00      0.99       107\n",
      "           2       1.00      1.00      1.00       270\n",
      "           3       1.00      1.00      1.00      1141\n",
      "\n",
      "    accuracy                           1.00      1851\n",
      "   macro avg       0.99      1.00      1.00      1851\n",
      "weighted avg       1.00      1.00      1.00      1851\n",
      "\n",
      "****************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_traffic_data(train_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "12d9c065a59539a14a3ee05dcce491e834d222100a37cc8927a456fa9268f310"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
